# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7seaP_XWkS7hWOfWtEvceNBa1HT0uAa
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.util import ngrams
from nltk.probability import FreqDist

nltk.download('punkt') # Reverting to standard 'punkt' resource
nltk.download('punkt_tab') # Added to address LookupError

# Sample text
text = "Language modeling is an important part of natural language processing"

# Tokenize words
tokens = word_tokenize(text.lower())

# Create bigrams
bigrams = list(ngrams(tokens, 2))

# Frequency distribution
freq = FreqDist(bigrams)

print("Bigram Frequencies:")
for k, v in freq.items():
    print(k, ":", v)

print("\nMost Common Bigrams:")
print(freq.most_common(5))

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Added to fix the LookupError

text = "Natural Language Processing helps computers understand human language"

tokens = word_tokenize(text)

stop_words = set(stopwords.words('english'))
filtered_words = [word for word in tokens if word.lower() not in stop_words]

ps = PorterStemmer()
stemmed_words = [ps.stem(word) for word in filtered_words]

print("Original Text:", text)
print("Tokens:", tokens)
print("After Stopword Removal:", filtered_words)
print("After Stemming:", stemmed_words)

from collections import Counter
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

text = "NLP is easy and NLP is powerful"

words = word_tokenize(text.lower())

word_freq = Counter(words)

print(word_freq)