# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7seaP_XWkS7hWOfWtEvceNBa1HT0uAa
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist

# Ensure you have the necessary data
nltk.download('punkt')
nltk.download('stopwords')

def analyze_word_freq(text):
    # 1. Tokenization: Convert text into a list of words
    raw_tokens = word_tokenize(text.lower())

    # 2. Cleaning: Remove punctuation and common "stop words"
    stop_words = set(stopwords.words('english'))

    # We use .isalnum() to keep only alphanumeric tokens (removes symbols)
    clean_tokens = [
        word for word in raw_tokens
        if word.isalnum() and word not in stop_words
    ]

    # 3. Frequency Calculation
    fdist = FreqDist(clean_tokens)

    return fdist

# Example Text (The beginning of 'The Great Gatsby')
text_data = """
In my younger and more vulnerable years my father gave me some advice
that I've been turning over in my mind ever since. Whenever you feel
like criticizing anyone, he told me, just remember that all the people
in this world haven't had the advantages that you've had.
"""

results = analyze_word_freq(text_data)

# Print the 5 most common words and their counts
print("Top 5 Words:")
for word, frequency in results.most_common(5):
    print(f"{word}: {frequency}")